---
title: Career-AI-Assistant
app_file: app.py
sdk: gradio
sdk_version: 5.49.1
---
# ğŸ¤– Career AI Assistant

An intelligent AI-powered career chatbot with RAG, semantic caching, and an embeddable portfolio widget. Uses Groq's Llama 3.3 70B for blazing-fast, free inference. Deployed on HuggingFace Spaces.

[![Live Demo](https://img.shields.io/badge/ğŸš€%20Live%20Demo-HuggingFace-blue)](https://huggingface.co/spaces/Cyclostone5945/Career-AI-Assistant)
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Groq](https://img.shields.io/badge/Groq-Llama%203.3%2070B-green.svg)](https://groq.com/)
[![Gradio](https://img.shields.io/badge/Gradio-5.0+-orange.svg)](https://gradio.app/)

---

## ğŸŒŸ Features

- **ğŸ§  RAG Pipeline** - ChromaDB vector database with semantic search for accurate, context-aware responses
- **ğŸ’° Semantic Caching** - DiskCache-based caching reduces API calls and costs by serving repeated queries instantly
- **ï¿½ Embeddable Widget** - Beautiful chat widget with FastAPI backend for portfolio integration
- **ï¿½ğŸ¯ Personalized Responses** - Trained on LinkedIn profile and career documents
- **ğŸ“§ Lead Capture** - Automatically records visitor contact information via tool calling
- **ğŸ’¾ Persistent Storage** - SQLite database for leads, knowledge gaps, and cache analytics
- **ğŸ”” Real-time Notifications** - Instant push notifications via Pushover
- **âš¡ Blazing Fast** - Powered by Groq's LPU hardware for millisecond inference
- **ğŸ†“ Zero API Cost** - Uses Groq's free tier with Llama 3.3 70B model

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              User Interfaces                                  â”‚
â”‚   Gradio Chat UI  â”‚  Embeddable Widget  â”‚  FastAPI Docs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Core Chat Service                            â”‚
â”‚  â€¢ Semantic cache check (DiskCache)                          â”‚
â”‚  â€¢ RAG context retrieval (ChromaDB)                          â”‚
â”‚  â€¢ Dynamic system prompt construction                        â”‚
â”‚  â€¢ Tool calling (lead capture, knowledge gaps)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Groq LPU Cloud (via OpenAI-compatible API)       â”‚
â”‚  â€¢ Model: llama-3.3-70b-versatile                            â”‚
â”‚  â€¢ Function calling enabled                                   â”‚
â”‚  â€¢ Free tier - no API costs                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Storage Layer                                 â”‚
â”‚  SQLite (leads, gaps, analytics)  â”‚  DiskCache  â”‚  ChromaDB  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
project_career_ai_assistant/
â”œâ”€â”€ app.py                      # Gradio entry point
â”œâ”€â”€ api_server.py               # FastAPI server for widget
â”œâ”€â”€ config.py                   # Configuration & Groq client
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md
â”œâ”€â”€ .env                        # Environment variables (not in repo)
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ core/                       # Core chat logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chat.py                 # Conversation logic with RAG + caching
â”‚   â””â”€â”€ tools.py                # AI tool functions (lead capture, etc.)
â”‚
â”œâ”€â”€ rag/                        # RAG pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_store.py         # ChromaDB operations
â”‚   â”œâ”€â”€ knowledge_indexer.py    # Document chunking & indexing
â”‚   â””â”€â”€ retriever.py            # Semantic search retrieval
â”‚
â”œâ”€â”€ storage/                    # Data persistence
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py             # SQLite operations
â”‚   â””â”€â”€ cache.py                # Semantic caching (DiskCache)
â”‚
â”œâ”€â”€ utils/                      # Utility scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ view_data.py            # Admin data viewer
â”‚
â”œâ”€â”€ widget/                     # Embeddable portfolio widget
â”‚   â””â”€â”€ chat-widget.html        # Standalone chat widget
â”‚
â””â”€â”€ data/                       # Runtime data (gitignored)
    â”œâ”€â”€ knowledge/              # Knowledge base documents
    â”‚   â”œâ”€â”€ linkedin.pdf
    â”‚   â””â”€â”€ summary.txt
    â”œâ”€â”€ cache/                  # DiskCache storage
    â””â”€â”€ chroma_db/              # ChromaDB vector database
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Groq API key (free at [console.groq.com](https://console.groq.com))
- (Optional) Pushover account for notifications

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Cyclostone/career-ai-assistant.git
   cd career-ai-assistant
   ```

2. **Create virtual environment & install dependencies**
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

3. **Set up environment variables**
   
   Create a `.env` file in the project root:
   ```bash
   GROQ_API_KEY=gsk_your_groq_api_key_here
   PUSHOVER_USER=your-pushover-user-key    # Optional
   PUSHOVER_TOKEN=your-pushover-token      # Optional
   ```

4. **Add your knowledge files**
   
   Place documents in `data/knowledge/`:
   - `linkedin.pdf` - Export your LinkedIn profile as PDF
   - `summary.txt` - Career summary
   - Any `.pdf`, `.txt`, or `.md` files

5. **Index the knowledge base**
   ```bash
   python -m rag.knowledge_indexer
   ```

6. **Run the Gradio app**
   ```bash
   python app.py
   ```
   Open http://127.0.0.1:7860 in your browser

7. **Run the FastAPI server** (for widget)
   ```bash
   python api_server.py
   ```
   API docs at http://127.0.0.1:8000/docs

---

## ğŸ¨ Embeddable Widget

Drop the chat widget into any portfolio website:

1. Start the FastAPI server: `python api_server.py`
2. Open `widget/chat-widget.html` in a browser
3. Update `API_URL` in the widget to point to your deployed server

The widget features:
- Modern UI with typing indicators
- Conversation history
- Error handling with retry
- Fully responsive design

---

## ğŸ› ï¸ How It Works

### 1. RAG Pipeline
```
User Query â†’ ChromaDB Semantic Search â†’ Top-K Relevant Chunks â†’ Context for LLM
```
- Documents are chunked (500 chars, 50 overlap) and embedded in ChromaDB
- Cosine similarity search retrieves the most relevant context
- Source attribution included in responses

### 2. Semantic Caching
```
User Query â†’ Generate Cache Key â†’ Check DiskCache â†’ HIT: Return cached | MISS: Call LLM
```
- SHA256 hash of query + context as cache key
- 7-day TTL with LRU eviction
- Cache analytics tracked in SQLite

### 3. Conversation Loop
```python
# Check cache first
cached = get_cached_response(query, context)
if cached:
    return cached['response']

# RAG-augmented LLM call
response = groq_client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=messages,
    tools=tools
)
```

### 4. Tool Execution
- **Capture a lead** â†’ `record_user_details(email, name, notes)` â†’ SQLite + Push notification
- **Log unknown question** â†’ `record_unknown_question(question)` â†’ SQLite + Push notification

---

## ğŸ“Š Admin Tools

```bash
# View database statistics
python -m utils.view_data stats

# View all captured leads
python -m utils.view_data leads

# View knowledge gaps
python -m utils.view_data gaps

# View cache statistics
python -c "from storage.cache import get_cache_stats; print(get_cache_stats())"

# View cache analytics
python -c "from storage.database import get_cache_analytics; print(get_cache_analytics())"
```

---

## ğŸ”§ Configuration

### Model Settings (`config.py`)

```python
# Groq client (OpenAI-compatible API)
openai_client = OpenAI(
    api_key=os.getenv("GROQ_API_KEY"),
    base_url="https://api.groq.com/openai/v1"
)

MODEL = "llama-3.3-70b-versatile"  # Free Groq model
ASSISTANT_NAME = "Arpit Shrotriya"
KNOWLEDGE_DIR = "data/knowledge"
DATABASE_PATH = "data/leads.db"
VECTOR_DB_DIR = "data/chroma_db"
```

---

## ğŸ” Security

- API keys stored in environment variables (never committed)
- `.env` file gitignored
- HuggingFace Spaces secrets encrypted
- CORS configured for widget API
- Input validation on all endpoints

---

## ğŸ“ˆ Development Phases

### Phase 1: Core Chat Bot
- Gradio chat interface
- OpenAI GPT-4o-mini integration
- LinkedIn PDF knowledge loading
- Tool calling for lead capture

### Phase 2: Database Integration
- SQLite persistent storage
- Lead management & knowledge gap tracking
- Push notifications via Pushover
- Admin viewer script

### Phase 3: RAG, Caching & Widget (Current)
- ChromaDB vector database with semantic search
- DiskCache semantic caching for cost optimization
- Embeddable portfolio widget with FastAPI backend
- Migration to Groq Llama 3.3 70B (free inference)
- Organized modular codebase

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Built as part of the [Agentic AI Course](https://github.com/ed-donner/agents)
- Inspired by Ed Donner's Lab 4 - Career Conversation project
- Powered by [Groq](https://groq.com/), [ChromaDB](https://www.trychroma.com/), [Gradio](https://gradio.app/), and [HuggingFace](https://huggingface.co/)

---

## ğŸ“ Contact

**Arpit Shrotriya**
- ğŸ“§ Email: arpit.shrotriya5945@gmail.com
- ğŸ¤– Try the AI: [Career AI Assistant](https://huggingface.co/spaces/Cyclostone5945/Career-AI-Assistant)

---

**Made with â¤ï¸ by Arpit Shrotriya**
